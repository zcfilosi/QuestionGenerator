import nltk
from nltk.stem.snowball import SnowballStemmer
import wikipedia
import random
'''
# uncomment if never used before
nltk.download('averaged_perceptron_tagger')
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
'''

def cards_for_terms(in_text):
    text = in_text

    # Used when tokenizing words
    sentence_re = r'''(?x)      # set flag to allow verbose regexps
          ([A-Z])(\.[A-Z])+\.?  # abbreviations, e.g. U.S.A.
        | \w+(-\w+)*            # words with optional internal hyphens
        | \$?\d+(\.\d+)?%?      # currency and percentages, e.g. $12.40, 82%
        | \.\.\.                # ellipsis
        | [][.,;"'?():-_`]      # these are separate tokens
    '''

    lemmatizer = nltk.WordNetLemmatizer()
    stemmer = nltk.stem.porter.PorterStemmer()
    otherstemmer = SnowballStemmer("english")
    #Taken from Su Nam Kim Paper...
    grammar = r"""
        NBAR:
            {<NN.*|JJ>*<NN.*>}  # Nouns and Adjectives, terminated with Nouns

        NP:
            {<NBAR>}
            {<NBAR><IN><NBAR>}  # Above, connected with in/of/etc...
    """
    chunker = nltk.RegexpParser(grammar)

    toks = nltk.word_tokenize(text)
    postoks = nltk.tag.pos_tag(toks)

    print(postoks)

    tree = chunker.parse(postoks)

    from nltk.corpus import stopwords
    stopwords = stopwords.words('english')


    def leaves(tree):
        """Finds NP (nounphrase) leaf nodes of a chunk tree."""
        for subtree in tree.subtrees(filter = lambda t: t.label()=='NP'):
            yield subtree.leaves()

    def normalise(word):
        """Normalises words to lowercase and stems and lemmatizes it."""
        word = word.lower()
        #word = SnowballStemmer("english", ignore_stopwords = True).stem(word)
        word = lemmatizer.lemmatize(word)
        return word

    def acceptable_word(word):
        """Checks conditions for acceptable word: length, stopword."""
        accepted = bool(2 <= len(word) <= 60
            and word.lower() not in stopwords)
        return accepted


    def get_terms(tree):
        for leaf in leaves(tree):
            term = [ normalise(w) for w,t in leaf if acceptable_word(w) ]
            yield term

    terms = get_terms(tree)
    '''
    for term in terms:
        for word in term:
            print(word),
    print()
    '''
    output_list = []
    search_term_list = []
    for term in terms:
        sterm = " ".join(term)
        if(sterm in search_term_list):
            continue
        search_term_list.append(sterm)
        try:
            suggestedterm = wikipedia.summary(sterm, sentences=1)
        except wikipedia.DisambiguationError as e:
            topic = e.options[0]
            suggestedterm = wikipedia.summary(topic, sentences = 1)
            #print("Reccomendation may refer to: ")
            #for i, topic in enumerate(topics):
                #print (str(i) + " " + topic)
            #choice = int(input("Enter a choice: "))
            #assert choice in range(len(topics))
            #suggestedterm = wikipedia.summary(topics[choice], sentences=1)
        except wikipedia.PageError:
            suggestedterm = " Nothing found For This Term"
        print(sterm + ": " + suggestedterm)
        output_list.append([sterm, suggestedterm])
        search_term_list.append(" ".join(term))
