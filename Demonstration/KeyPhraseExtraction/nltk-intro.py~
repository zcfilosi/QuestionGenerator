import nltk
from nltk.stem.snowball import SnowballStemmer
import wikipedia
import random
'''
# uncomment if never used before
nltk.download('averaged_perceptron_tagger')
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
'''
text = """In mathematics, computer science, and linguistics, a formal language consists of words whose letters are taken from an alphabet and are well-formed according to a specific set of rules.
The alphabet of a formal language consist of symbols, letters, or tokens that concatenate into strings of the language.[1] Each string concatenated from symbols of this alphabet is called a word, and the words that belong to a particular formal language are sometimes called well-formed words or well-formed formulas. A formal language is often defined by means of a formal grammar such as a regular grammar or context-free grammar, which consists of its formation rules. """

# Used when tokenizing words
sentence_re = r'''(?x)      # set flag to allow verbose regexps
      ([A-Z])(\.[A-Z])+\.?  # abbreviations, e.g. U.S.A.
    | \w+(-\w+)*            # words with optional internal hyphens
    | \$?\d+(\.\d+)?%?      # currency and percentages, e.g. $12.40, 82%
    | \.\.\.                # ellipsis
    | [][.,;"'?():-_`]      # these are separate tokens
'''

lemmatizer = nltk.WordNetLemmatizer()
stemmer = nltk.stem.porter.PorterStemmer()
otherstemmer = SnowballStemmer("english")
#Taken from Su Nam Kim Paper...
grammar = r"""
    NBAR:
        {<NN.*|JJ>*<NN.*>}  # Nouns and Adjectives, terminated with Nouns
        
    NP:
        {<NBAR>}
        {<NBAR><IN><NBAR>}  # Above, connected with in/of/etc...
"""
chunker = nltk.RegexpParser(grammar)

toks = nltk.word_tokenize(text)
postoks = nltk.tag.pos_tag(toks)

print(postoks)

tree = chunker.parse(postoks)

from nltk.corpus import stopwords
stopwords = stopwords.words('english')


def leaves(tree):
    """Finds NP (nounphrase) leaf nodes of a chunk tree."""
    for subtree in tree.subtrees(filter = lambda t: t.label()=='NP'):
        yield subtree.leaves()

def normalise(word):
    """Normalises words to lowercase and stems and lemmatizes it."""
    word = word.lower()
    #word = SnowballStemmer("english", ignore_stopwords = True).stem(word)
    word = lemmatizer.lemmatize(word)
    return word

def acceptable_word(word):
    """Checks conditions for acceptable word: length, stopword."""
    accepted = bool(2 <= len(word) <= 60
        and word.lower() not in stopwords)
    return accepted


def get_terms(tree):
    for leaf in leaves(tree):
        term = [ normalise(w) for w,t in leaf if acceptable_word(w) ]
        yield term

terms = get_terms(tree)
'''
for term in terms:
    for word in term:
        print(word),
print()
'''
search_term_list = []
for term in terms:
    sterm = " ".join(term)
    if(sterm in search_term_list):
        continue
    search_term_list.append(sterm)
    try:
        suggestedterm = wikipedia.summary(sterm, sentences=1)
    except wikipedia.DisambiguationError as e:
        topic = e.options[0]
        suggestedterm = wikipedia.summary(topic, sentences = 1)
        #print("Reccomendation may refer to: ")
        #for i, topic in enumerate(topics):
            #print (str(i) + " " + topic)
        #choice = int(input("Enter a choice: "))
        #assert choice in range(len(topics))
        #suggestedterm = wikipedia.summary(topics[choice], sentences=1)
    except wikipedia.PageError:
        suggestedterm = " Nothing found For This Term"
    print(sterm + ": " + suggestedterm)
    search_term_list.append(" ".join(term))
